---
title: "Practical Machine Learning Course Project"
author: "David Lennox-Hvenekilde"
date: "03/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning=FALSE)
```

## Introduction to the data
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

he training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

### Load the data:
```{r loaddata}
# Make sure the data is in the same directory as this .rmd
dir_name <-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(dir_name)
# Load data
pml.testing <- read.csv("pml-testing.csv", row.names = 1)
pml.training <- read.csv("pml-training.csv", row.names = 1)
# Give an overview of the data
dim(pml.training)
dim(pml.testing)
```
The training data contains 19622 observations of 159 variables.
The test data contains 20 observations.

The goal of the project is to predict the manner in which participants did the exercise. This is the "classe" variable in the training set. Let's have a look at the "classe" variable:
```{r classe}
library(ggplot2)
ggplot(data = pml.training, aes(classe, ..count..))+geom_bar(aes(fill = classe))
```
It seems that the classe variable contains an outcome of 5 possibilities, that appears at a relatively similar frequency, except classe "A" which seems more prevalent than the others.

### Splitting up the data
First we split the training dataset into a training and a validation set, in order to calculate the out of sample error down the line.

```{r split}
library(caret)
set.seed(3980)
inTrain <- createDataPartition(y=pml.training$classe, p=0.8, list=F)
pml.train <- pml.training[inTrain, ]
pml.validate <- pml.training[-inTrain, ]
```

### Cleaning up the data
The data has a lot or variables. To reduce the number we check if any have near zero variance using the caret function. Those that do, we remove from the training dataset:

```{r nzv}
nsv <- nearZeroVar(pml.train, saveMetrics = TRUE)
head(nsv)

# Take the rows (columns in original data) that have near zero variance prediction
nzvp_rm<-rownames(nsv[nsv$nzv==TRUE,])

# Remove these variables from the training data
pml.train<-pml.train[,-which(colnames(pml.train) %in% nzvp_rm)]
dim(pml.train)
```

56 variables with near zero variance were removed, giving a training data set with 103 variables. After further inspection a lot of the column seem to have mainly NAs. Here we identify and remove those with > 80% NA values:

```{r NArm}
NaColNames<-names(pml.train)[sapply(pml.train, function(x) sum(is.na(x)) > length(x)*0.8)]
pml.train<-pml.train[,-which(colnames(pml.train) %in% NaColNames)]
dim(pml.train)
```
Another 45 variables have been removed, leaving 58

### Model building
As we have categorical data that we want to predict (the "classe" variable) a powerful option is the random forest approach.

```{r RFmodel}
# We start with random forest, with 3x cross validation. Don't print the training log
Control <- trainControl(method="cv", number=3, verboseIter=F)

# I will not run the traing of the model again. to save time, it has been stored in the object "pmlRF.rda" provided in the repo.
#RFfit <- train(classe ~ ., data=pml.train, method="rf",trControl=Control)
#save(RFfit, file = "pmlRF.rda")
load("pmlRF.rda")
```
Using the validation dataset that we split from our original training dataset, we can asses this model with a confusion matrix, giving an indication of out of sample accuracy:

```{r RFconfuse}
pred_RF <- predict(RFfit, pml.validate)
cmRF <- confusionMatrix(pred_RF, factor(pml.validate$classe))
cmRF
```
Suprisingly, this model has a 100% accuracy. Rather than develop more on our prediction model, we will use this initial Random Forest model to predict on our testing dataset:

```{r RFtest}
test_pred_RF <- predict(RFfit, pml.testing)
test_pred_RF
```
We now have our 20 predictions of "classe" for our testing dataset. Plugging these into the project prediction quiz shows that they are all 20 correct.  